{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e599a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell contains the essential imports you will need – DO NOT CHANGE THE CONTENTS! ##\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e2f12a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complex_Normalize(z):\n",
    "    re = z.real\n",
    "    im = z.imag\n",
    "    re = F.normalize(re)\n",
    "    im = F.normalize(im)\n",
    "    \n",
    "    return torch.complex(re, im)\n",
    "\n",
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1),\n",
    "    #torch.fft.fft2,\n",
    "    #torch.fft.fftshift,\n",
    "    #Complex_Normalize\n",
    "])\n",
    "\n",
    "valid_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1),\n",
    "    #torch.fft.fft2,\n",
    "    #torch.fft.fftshift,\n",
    "    #Complex_Normalize\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0, 1),\n",
    "    #torch.fft.fft2,\n",
    "    #torch.fft.fftshift,\n",
    "    #Complex_Normalize\n",
    "])\n",
    "\n",
    "\n",
    "# Create training set (with no transformations)\n",
    "raw_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=None)\n",
    "\n",
    "# Split the training data into pure training data and validation data (80/20 split)\n",
    "train_size = int(len(raw_data) * 0.6) # 60% training data\n",
    "valid_size = int(len(raw_data) * 0.2) # 20% validation data\n",
    "test_size = len(raw_data)-(train_size+valid_size)\n",
    "\n",
    "training_data, validation_data, testing_data = torch.utils.data.random_split(raw_data, [train_size, valid_size, test_size])\n",
    "\n",
    "# Apply the respective transformations to each dataset\n",
    "training_data.dataset.transform = train_transforms\n",
    "validation_data.dataset.transform = valid_transforms\n",
    "testing_data.dataset.transform = test_transforms\n",
    "\n",
    "# Create test set and define test dataloader\n",
    "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(validation_data, batch_size=32)\n",
    "test_loader = DataLoader(testing_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "89889218",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Explore data\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m show_image_and_fft(f)\n",
      "Cell \u001b[0;32mIn[235], line 14\u001b[0m, in \u001b[0;36mshow_image_and_fft\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_image_and_fft\u001b[39m(image):\n\u001b[0;32m---> 14\u001b[0m     fft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfft2(image)\n\u001b[1;32m     15\u001b[0m     fft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(fft)\n\u001b[1;32m     16\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got -2)"
     ]
    }
   ],
   "source": [
    "## This cell contains a function for showing 5 images from a dataloader – DO NOT CHANGE THE CONTENTS! ##\n",
    "this_batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "x = np.linspace(-50, 50, 100)\n",
    "y = np.linspace(-50, 50, 100)\n",
    "#X, Y = np.meshgrid(x, y)\n",
    "f_xy = np.heaviside(x-2,1)*np.heaviside(-x+10,1)*np.exp(-y**2/500)\n",
    "\n",
    "f = torch.Tensor(f_xy)\n",
    "\n",
    "\n",
    "def show_image_and_fft(image):\n",
    "    fft = torch.fft.fft2(image)\n",
    "    fft = torch.fft.fftshift(fft)\n",
    "    image = image.numpy()\n",
    "    fft = fft.numpy()\n",
    "    fft_real = fft.real\n",
    "    fft_imag = fft.imag\n",
    "    fft_abs = np.abs(fft)\n",
    "\n",
    "    #show the imaginary part of the image\n",
    "    plt.imshow(image.T.squeeze().T)\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(fft_real.T.squeeze().T)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(fft_imag.T.squeeze().T)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.imshow(fft_abs.T.squeeze().T)\n",
    "    plt.show()\n",
    "\n",
    "def show(batch, n=1):\n",
    "    labels = batch[1][0:n]\n",
    "    images = batch[0][0:n]\n",
    "    #show inverse images and ffts\n",
    "    for i in range(n):\n",
    "        print(int(labels[i].detach()))\n",
    "        \n",
    "        inverted_image = torch.fft.fftshift(images[i])\n",
    "        inverted_image = torch.fft.ifft2(images[i]).abs().numpy()\n",
    "\n",
    "        image = transforms.functional.rotate(images[i], 90)\n",
    "        image = image.abs().numpy()\n",
    "        \n",
    "        #show the imaginary part of the image\n",
    "        plt.imshow(inverted_image.T.squeeze().T)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.imshow(image.T.squeeze().T)\n",
    "        plt.show()\n",
    "        \n",
    "# Explore data\n",
    "show_image_and_fft(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
